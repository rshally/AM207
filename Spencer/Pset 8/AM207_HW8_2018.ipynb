{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Spring 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Friday, March 30th, 2018 at 11:00am\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Problem 1: Understanding Yelp Review Data As a Human\n",
    "In this course, we've spent a lot of time learning algorithms for performing inference on complex models and we've spent time using these models to make decisions regarding our data. But in nearly every assignment, the model for the data is specified in the problem statement. In real life, the creative and, arguably, much more difficult task is to start with a broadly defined goal and then to customize or create a model which will meet this goal in some way. \n",
    "\n",
    "\n",
    "\n",
    "Problem #1 is atypical in that it does not involve any programming or (necessarily) difficult mathematics/statistics. The process of answering these questions *seriously* will however give you an idea of how one might create or select a model for a particular application and your answers will help you with formalizing the model if and when you're called upon to do so.\n",
    "\n",
    "***Grading:*** *We want you to make a genuine effort to mold an ambiguous and broad real-life question into a concrete data science or machine learning problem without the pressure of getting the \"right answer\". As such, we will grade your answer of Problem #1 on a pass/fail basis. Any reasonable answer that demonstrates actual effort will be given a full grade.*\n",
    "\n",
    "We've compiled for you a fairly representative selection of [Yelp reviews](./yelp_reviews.zip) for a (now closed) sushi restaurant called Ino's Sushi in San Francisco. Read the reviews and form an opinion regarding the various qualities of Ino's Sushi. Answer the following:\n",
    "\n",
    "1. If the task is to summarize the quality of a restaurant in a simple and intuitive way, what might be problematic with simply classifying this restaurant as simply \"good\" or \"bad\"? Justify your answers with specific examples from the dataset.\n",
    "\n",
    "2. For Ino's Sushi, categorize the food and the service, separately, as \"good\" or \"bad\" based on all the reviews in the dataset. Be as systematic as you can when you do this.\n",
    "\n",
    "  (**Hint:** Begin by summarizing each review. For each review, summarize the reviewer's opinion on two aspects of the restaurant: food and service. That is, generate a classification (\"good\" or \"bad\") for each aspect based on what the reviewer writes.) \n",
    "  \n",
    "3. Identify statistical weaknesses in breaking each review down into an opinion on the food and an opinion on the service. That is, identify types of reviews that make your method of summarizing the reviewer's optinion on the quality of food and service problemmatic, if not impossible. Use examples from your dataset to support your argument. \n",
    "\n",
    "4. Identify all the ways in which the task in #2 might be difficult for a machine to accomplish. That is, break down the classification task into simple self-contained subtasks and identify how each subtask can be accomplished by a machine (i.e. which area of machine learning, e.g. topic modeling, sentiment analysis etc, addressess this type of task).\n",
    "\n",
    "5. Describe a complete pipeline for processing and transforming the data to obtain a classification for both food and service for each review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: My Sister-In-Law's Baby Cousin Tracy ...\n",
    "\n",
    "\n",
    "Wikipedia describes the National Annenberg Election Survey as follows -- \"National Annenberg Election Survey (NAES) is the largest academic public opinion survey conducted during the American presidential elections. It is conducted by the Annenberg Public Policy Center at the University of Pennsylvania.\"  In the file [survey.csv](./survey.csv) we provide the following data from the 2004 National Annenberg Election Survey:  `age` -- the age of the respondents, `numr` -- the number of responses, and `knowlgbtq` -- the number of people at the given age who have at least one LGBTQ acquaintance.  We want you to model how age influences likelihood of interaction with members of the LGBTQ community in three ways. \n",
    "\n",
    "\n",
    "1. Using pymc3, create a bayesian regression model (either construct the model directly or use the glm module) with the same feature and dependent variable. Plot the mean predictions for ages 0-100, with a 2-sigma envelope.\n",
    "\n",
    "2. Using pymc3, create a 1-D Gaussian Process regression model with the same feature and dependent variables.  Use a squared exponential covariance function. Plot the mean predictions for ages 0-100, with a 2-sigma envelope.\n",
    "\n",
    "3. How do the models compare? Does age influence likelihood of acquaintance with someone LGBTQ? For Bayesian Linear Regression and GP Regression, how does age affect the variance of the estimates?\n",
    "\n",
    "For GP Regression, we can model the likelihood of knowing someone LGBTQ as a product of binomials -- one binomial distribution per age group. \n",
    "\n",
    "$$p(y_a | \\theta_a, n_a) = Binom( y_a, n_a, \\theta_a)$$\n",
    "\n",
    "where $y_a$ (i.e. `knowlgbtq`) is the observed number of respondents who know someone lgbtq  at age $a$, $n_a$ (i.e. `numr`) is the number of trials and $\\theta_a$ is the rate parameter for having an lgbtq acquaintance at age $a$.\n",
    "\n",
    "Using the Gaussian approximation  (http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation) to approximate the Binomial since `numr` is large, you can simply use a GP posterior with the error for each measurement to be given using this approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:  Like a Punch to the Kidneys \n",
    "\n",
    "In this problem we will work with the US Kidney Cancer Dataset (by county), a dataset of kidney cancer frequencies across the US over 5 years on a per county basis. \n",
    "\n",
    "The kidney cancer data can be found [here](./kcancer.csv).\n",
    "\n",
    "A casual inspection of the data might suggest that we independently model cancer rates for each of the provided counties.  Our experience in past homeworks/labs/lectures (in particular when we delved into the Rat Tumors problem) suggests potential drawbacks of conclusions based on raw cancer rates.  Addressing these drawbacks, let's look use a Bayesian model for our analysis of the data. In particular you will implement an Empircal Bayes model to examine the adjusted cancer rates per county.\n",
    "\n",
    "Let $N$ be the number of counties; let $y_j$ the number of kidney cancer case for the $j$-th county, $n_j$ the population of the $j$-th county and $\\theta_j$ the underlying kidney cancer rate for that county. We can construct a Bayesian model for our data as follows:\n",
    "\\begin{aligned}\n",
    "y_j &\\sim Poisson(5 \\cdot n_j \\cdot \\theta_j), \\quad j = 1, \\ldots, N\\\\\n",
    "\\theta_j &\\sim Gamma(\\alpha, \\beta), \\quad j = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "where $\\alpha, \\beta$ are hyper-parameters of the model.\n",
    "\n",
    "- (#1) Implement Empirical Bayes via moment matching as described as follows. Consider the **prior-predictive** distribution (also called the evidence i.e. the denominator normalization in bayes theorem) of the model: $p(y) = \\int p(y \\vert \\theta) p(\\theta) d \\theta$. Why the prior-predictive? Because technically we \"haven't seen\" individual county data yet.  For this model, the prior-predictive is a negative binomial. Matching the mean and the variance of the negative binomial to that from the data, you can find appropriate expressions for $\\alpha$ and $\\beta$. (Hint: You need to be careful with the $5n_j$ multiplier.) \n",
    "\n",
    "- (#2) Produce a scatter plot of the raw cancer rates (pct mortality) vs the county population size. Highlight the top 300 raw cancer rates in red. Highlight the bottom 300 raw cancer rates in blue. Finally, on the same plot add a scatter plot visualization of the posterior mean cancer rate estimates (pct mortality) vs the county population size, highlight these in green.\n",
    "\n",
    "- (#3) Using the above scatter plot, explain why using the posterior means from our model to estimate cancer rates is preferable to studying the raw rates themselves.\n",
    "\n",
    "(**Hint:** You might also find it helpful to follow the Rat Tumor example.)\n",
    "\n",
    "(**Note:** Up until now we've had primarily thought about the posterior predictive: $\\int p( y \\vert \\theta) p(\\theta \\vert D) d\\theta$.  The posterior predictive and the prior predictive can be somewhat connected. In conjugate models such as ours, the two distributions have the same form.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: In the Blink of a Bayesian Iris\n",
    "\n",
    "We've done classification before, but the goal of this problem is to introduce you to the idea of classification using Bayesian inference. \n",
    "\n",
    "Consider the famous *Fisher flower Iris data set* a  multivariate data set introduced by Sir Ronald Fisher (1936) as an example of discriminant analysis. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, you will build a model to predict the species. \n",
    "\n",
    "For this problem only consider two classes: **virginica** and **not-virginica**. \n",
    "\n",
    "The iris data can be obtained [here](./iris.csv).\n",
    "\n",
    "Let $(X, Y )$ be our dataset, where $X=\\{\\vec{x}_1, \\ldots \\vec{x}_n\\}$ and $\\vec{x}_i$ is the standard feature vector corresponding to an offset 1 and the four components explained above. $Y \\in \\{0,1\\}$ are the scalar labels of a class. In other words the species labels are your $Y$ data (virginica = 0 and virginica=1), and the four features -- petal length, petal width, sepal length and sepal width -- along with the offset make up your $X$ data. \n",
    "\n",
    "The goal is to train a classifier, that will predict an unknown class label $\\hat{y}$ from a new data point $x$. \n",
    "\n",
    "Consider the following glm (logistic model) for the probability of a class:\n",
    "\n",
    "$$ p(y) = \\frac{1}{1+e^{-x^T \\beta}} $$\n",
    "\n",
    "(or $logit(p) = x^T \\beta$ in more traditional glm form)\n",
    "\n",
    "where $\\beta$ is a 5D parameter to learn. \n",
    "\n",
    "Then given $p$ at a particular data point $x$, we can use a bernoulli likelihood to get 1's and 0's. This should be enough for you to set up your model in pymc3. (Other Hints: also use theano.tensor.exp when you define the inverse logit to go from $\\beta$ to $p$, and you might want to set up $p$ as a deterministic explicitly so that pymc3 does the work of giving you the trace).\n",
    "\n",
    "Use a 60-40 stratified (preserving class membership) split of the dataset into a training set and a test set. (Feel free to take advantage of scikit-learn's `train_test_split`).\n",
    "\n",
    "1. Choose a prior for $\\beta \\sim N(0, \\sigma^2 I) $ and write down the formula for the normalized posterior $p(\\beta| Y,X)$. Since we dont care about regularization here, just use the mostly uninformative value $\\sigma = 10$.\n",
    "2. Find the MAP and mean estimate for the posterior on the training set.\n",
    "3. Implement a  sampler to sample from this posterior of $\\beta$.   Generate samples of $\\beta$ and plot the sequence of $\\beta$'s  and histograms for each $\\beta$ component.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
