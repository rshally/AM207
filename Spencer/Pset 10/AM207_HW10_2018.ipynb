{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spencer Hallyb\n",
    "\n",
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework 10\n",
    "**Harvard University**<br>\n",
    "**Spring 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Friday, April 13th, 2018 at 10:59am\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as an iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #1: Iris Eyes Are Smiling\n",
    "\n",
    "This Homework problem is a continuation of Problem #4 from Homework 8.  We'll be using the same iris dataset. The iris data can be obtained [here](./iris.csv).\n",
    "\n",
    "Now that we have from Homework 8 Problem #4 a train-test split as well as point estimate(s) and a posterior distribution on the probability of a data point being virginica, we can use these to make predictions on both the training set and test set.\n",
    "\n",
    "** *Note this next section is just guidance and doesn't contain any work for you to do* **\n",
    "\n",
    "*There are two ways to make these predictions, given an estimate of $p(y=1\\ \\vert\\ x)$:* \n",
    "\n",
    "(A) You can sample from the Bernoulli likelihood at the data point $x$ to decide if that particular data points classification $y(x)$ should be a 1 or a 0.\n",
    "\n",
    "(B) Or you could do the intuitive \"machine-learning-decision-theoretic\" (MLDT) thing, where you assign a data  point $x$ a classification 1 if $p(y=1 \\vert x) > 0.5$.\n",
    "\n",
    "*There are multiple ways in which you can do these probability estimates at a sample $x$:*\n",
    "\n",
    "(A) You can just use a point estimate like the MAP value, ($p_{MAP}$), or the posterior mean ($p_{MEAN}$)\n",
    "\n",
    "(B) You can see what fraction of your posterior samples have values above 0.5 (ie you are calculating 1-cdf(0.5)) on the posterior ($p_{CDF}$))\n",
    "\n",
    "Both these above methods miss the combined smearing of the posterior and sampling distributions. In other words they dont sample from the posterior predictive. If we draw a large number of samples from the posterior predictive distribution at a data point $x$, the fraction of 1s will give an estimate for the probability to use, $p_{PP}$, which is different from the MAP estimate, or the CDF estimate.\n",
    "\n",
    "** *Note this next section contains the tasks we expect you to complete* **\n",
    "\n",
    "\n",
    "1. Plot the distribution of $p_{MEAN}$, $p_{CDF}$, and $p_{PP}$ over all the data points in the training set. How are these different?\n",
    "2. Plot the posterior-predictive distribution of the misclassification rate with respect to the true class identities $y(x)$ of the data points $x$ (in other words you are plotting a histogram with the misclassification rate for the $n_{trace}$ posterior-predictive samples) on the training set.\n",
    "3. Make the same plot from the posterior, rather than the posterior predictive, by using the MLDT defined above. Overlay this plot on the previous one. That is, for every posterior sample, consider whether the data point ought to be classified as a 1 or 0 from the $p>0.5 \\implies y=1$ decision theoretic prespective. Compare with your previous diagram. Which case (from posterior-predictive or from-posterior) has a wider mis-classification distribution? \n",
    "4. Is the classification at the true training data points (you can think of this as a bitstring '100101...' 90 characters long) represented in the posterior predictive trace? If so, how many times? Is it the most frequent string in the traces? Explain your conclusions.\n",
    "5. Repeat 2 and 3 for the test set, i.e. make predictions. Describe and interpret the widths of the distributions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: From the Ritz to the Rubble\n",
    "\n",
    "In this problem, much like in Long Homework 9, you will explore how to recast data, tasks and research questions from a variety of different contexts so that an existing model can be applied for analysis.\n",
    "\n",
    "Example 10.1.3 in \"Statistical Rethinking\", the excerpt of which is included in the data directory for this assignment, illustrates a study of the effect of an applicant's gender on graduate school admissions to six U.C. Berkeley departments through a comparison of four models. \n",
    "\n",
    "In this problem, you are given the data for the 1994 U.S. Census. The data has been processed so that only a subset of the features are present (for full dataset as well as the description see the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Census+Income)). You will be investigate the effect of gender on a person's yearly income in the dataset. In particular, we want to know how a person's gender effect the likelihood of their yearly salary being above or below \\$50k. \n",
    "\n",
    "First we want to aggregate the dataset by seven different categories. The categories we wish to consider are: \n",
    "- 4 year college degree\n",
    "- Some-college or two year academic college degree\n",
    "- High school\n",
    "- Professional, vocational school\n",
    "- Masters \n",
    "- Doctorate\n",
    "- Some or no high school\n",
    "Note that you might have to combine some of the existing education categories in your dataframe. For each category, we suggest that you only keep track of a count of the number of males and females who make above (and resp. below) $50k (see the dataset in Example 10.1.3). \n",
    "\n",
    "Following Example 10.1.3, build two models for the classification of an individual's yearly income (1 being above \\$50k and 0 being below), one of these models should include the effect of gender while the other should not. \n",
    "\n",
    "Replicate the analysis in 10.1.3 using your models; specifically, compute= wAIC scores and make a plot like Figure 10.5 (posterior check) to see how well your models fits the data. Using your analysis, discuss the effect of gender on income.\n",
    "\n",
    "Following Example 10.1.3, build two models for the classification of an individual's yearly income taking into account education. \n",
    "\n",
    "Replicate the analysis in 10.1.3 using your models; specifically, compute AIC/BIC scores and make a plot like Figure 10.6 (posterior check) to see how well your model fits the data. Using your analysis, discuss the effect of gender on income, taking into account an individual's education."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
