{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework 11\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Monday, April 23rd, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as an iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AM207 Cambridge Nursery\n",
    "\n",
    "A plant nursery in Cambridge is exprimentally cross-breeding two types of hibiscus flowers: blue and pink. The goal is to create an exotic flower whose petals are pink with a ring of blue on each. \n",
    "\n",
    "There are four types of child plant that can result from this cross-breeding: \n",
    "\n",
    "  - Type 1: blue petals\n",
    "  - Type 2: pink petals \n",
    "  - Type 3: purple petals\n",
    "  - Type 4: pink petals with a blue ring on each (the desired effect). \n",
    "\n",
    "Out of 197 initial cross-breedings, the nursery obtained the following distribution over the four types of child plants: \n",
    "$$Y = (y_1, y_2, y_3, y_4) = (125, 18, 20, 34)$$\n",
    "where $y_i$ represents the number of child plants that are of type $i$.\n",
    "\n",
    "The nursery then consulted a famed Harvard plant geneticist, who informed them that the probability of obtaining each type of child plant in any single breeding experiment is as follows:\n",
    "$$ \\frac{\\theta+2}{4}, \\frac{1-\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{\\theta}{4}.$$\n",
    "Unfortunately, the geneticist did not specify the quantity $\\theta$.\n",
    "\n",
    "Clearly, the nursery is interested in understanding how many cross-breeding they must perform, on average, in order to obtain a certain number of child plants with the exotic blue rings. To do this they must be able to compute $\\theta$. \n",
    "\n",
    "The owners of the nursery, being top students in AM207, decided to model the experiment in hopes of discovering $\\theta$ using the results from their 197 initial experiments. \n",
    "\n",
    "They chose to model the observed data using a multinomial model and thus calculated the likelihood to be:\n",
    "$$ p(y  \\vert  \\theta) \\propto (2+\\theta)^{y_1} (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "\n",
    "Being good Bayesians, they also imposed a prior on $\\theta$, $\\rm{Beta}(a, b)$.\n",
    "\n",
    "Thus, the posterior is:\n",
    "$$ p(\\theta \\vert  Y) = \\left( 2+\\theta \\right)^{y_1} (1-\\theta)^{y_2+y_3} \\, \\theta^{\n",
    "y_4} \\, \\theta^{a-1} \\, (1-\\theta)^{b-1}. $$\n",
    "\n",
    "If the nursery owners are able to sample from the posterior, they would be able to understand the distribution of $\\theta$ and make appropriate estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. Sampling using data augmentation\n",
    "\n",
    "Realizing that it would be difficult to sample from the posterior directly and after being repeatedly frustrated by attempts of Metropolis-Hastings and Gibbs sampling for this model, the nursery owners decided to augment their model and hopefully obtain a friendlier looking distribution that allows for easy sampling.\n",
    "\n",
    "They augment the data with a new variable $z$ such that:\n",
    "$$z + (y_1 - z) = y_1.$$\n",
    "That is, using $z$, we are breaking $y_1$, the number of type I child plants, into two subtypes. Let the probability of obtaining the two subtype be $1/2$ and $\\theta/4$, respectively. Now, we can interpret $y_1$ to be the total number of trials in a binomial trial. Thus, the new likelihood can be written as\n",
    "$$ p(y, z  \\vert  \\theta) \\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z}  (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "\n",
    "\n",
    "Derive the joint posterior $p(\\theta, z  \\vert  y)$ and sample from it using Gibbs sampling.\n",
    "\n",
    "Visualize the distribution of theta and, from this distribution, estimate the probability of obtaining a type 4 child plant (with the blue rings) in any cross-breeding experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Finding the MLE using Expectation Maximization\n",
    "\n",
    "Treat the augmented model as a latent variable model. \n",
    "\n",
    "### Part A. \n",
    "\n",
    "Write down an expression (up to unimportant constants - you must decide what unimportant means) for each of the following:\n",
    "\n",
    "(1) the observed data log likelihood\n",
    "\n",
    "(2) the complete(full) data log likelihood\n",
    "\n",
    "**Hint:** You should already have the observed data likelihood and the complete data likelihood from Problem 1, you just need to take their logs for this problem.\n",
    "\n",
    "(3) the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, or the expected complete(full) data log likelihood, defined by\n",
    "$$Q(\\theta, \\theta^{(t-1)}) = \\mathbb{E}_{Z  \\vert  Y=y, \\Theta = \\theta^{t-1}}[\\text{the complete data log likelihood}]$$\n",
    "\n",
    "In other words $Z  \\vert  Y=y, \\Theta = \\theta^{t-1}$ is $q(z, \\theta_{old})$ from lecture at the end of the E-step and $Q$ is the z-posterior expectation (at $\\theta_{old}$) of the full data log likelihood, which is the ELBO minus the entropy of $q$ (which being evaluated at $\\theta_{old}$ is not dependent on $\\theta$ and thus irrelevant for maximization).\n",
    "\n",
    "### Part B:\n",
    "\n",
    "We will maximize the likelihood through Expectation Maximization (EM). In order to preform EM, we must iterate through the following steps\n",
    "\n",
    "- (Expectation) Compute the Auxilary function, $Q(\\theta, \\theta^{t-1})$ (the expectation of the full data likelihood)\n",
    "- (Maximization) Compute $\\theta^{t} = \\text{argmax}_\\theta Q(\\theta, \\theta^{(t-1)})$\n",
    "\n",
    "Thus, you must compute exact formulae for the following:\n",
    "1. the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, for a given $\\theta^{(t-1)}$. That is, compute the expectation of the complete data log likelihood.\n",
    "2. $\\theta^{t}$, by maximizing the Auxilary function $Q(\\theta, \\theta^{(t-1)})$.\n",
    "\n",
    "**Hint:** You don't actually need to do any difficult optimization for the M-step. After taking the expectation of the complete data log likelihood in the E-step, match your $Q(\\theta, \\theta^{(t-1)})$ to the log pdf of a familiar distribution, then use the known formula for the mode of this distribution to optimize $Q(\\theta, \\theta^{(t-1)})$.\n",
    "\n",
    "Use these to **estimate the MLE** of $\\theta$ using EM (choose your own reasonable criterion for convergence)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
